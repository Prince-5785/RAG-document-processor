# LLM-Powered Document Processing System Configuration

# Embedding Model Configuration
embedding:
  model_name: "sentence-transformers/all-MiniLM-L6-v2"
  device: "cpu"  # Change to "cuda" if GPU available
  batch_size: 32

# LLM Configuration
llm:
  # This 'api' section is new and crucial
  api:
    # The actual key will be injected from .env by the pipeline
    model_sequence:
      - 'meta-llama/llama-4-maverick-17b-128e-instruct'
      - 'llama-3.3-70b-versatile'
      - 'gemma2-9b-it'
      - 'llama-3.1-8b-instant'
  
  local_model:
    model_name: null # Set to a model name like 'gemma:2b' for local fallback
    device: "cpu"

  generation_params:
    temperature: 0.5
    top_p: 0.9
    max_new_tokens_parsing: 250
    max_new_tokens_decision: 500

# Document Processing
document_processing:
  chunk_size: 1000
  chunk_overlap: 200
  separators: ["\n\n", "\n", " ", ""]
  
# Vector Store Configuration
vector_store:
  persist_directory: "./data/chroma_index"
  collection_name: "insurance_documents"
  distance_metric: "cosine"
  
# Retrieval Configuration
retrieval:
  top_k: 5
  score_threshold: 0.7
  
# UI Configuration (This was missing)
ui:
  title: "CodersHub Insurance Query Solver"
  max_file_size: 200  # MB
  supported_formats: ["pdf", "docx", "eml", "txt"]
  
# Logging
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
# Cache Configuration
cache:
  enable_embedding_cache: true
  cache_directory: "./data/cache"